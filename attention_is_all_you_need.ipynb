{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOjWX3yLKVGBMYmgX1tc3sd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChandrashekharGhanokar/attention_is_all_you_need/blob/main/attention_is_all_you_need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding\n"
      ],
      "metadata": {
        "id": "e-KSLOUKExSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Adds positional information to the input embeddings\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_size, max_len):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pe = torch.zeros(max_len, embed_size)  # Create a matrix of size (max_len, embed_size)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)  # Positions 0, 1, 2, ...\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_size))\n",
        "        self.pe[:, 0::2] = torch.sin(position * div_term)  # Apply sin to even positions\n",
        "        self.pe[:, 1::2] = torch.cos(position * div_term)  # Apply cos to odd positions\n",
        "        self.pe = self.pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :].to(x.device)  # Add positional encoding to input\n",
        "        return x"
      ],
      "metadata": {
        "id": "RDT4hMHvE-YI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Layer\n"
      ],
      "metadata": {
        "id": "pmaXS3uzFK1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single transformer encoder layer with attention and feed-forward network\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_size, heads, dropout=dropout)  # Multi-head attention\n",
        "        self.norm1 = nn.LayerNorm(embed_size)  # Layer normalization\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),  # FFN layer 1\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size)  # FFN layer 2\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output, _ = self.attention(x, x, x, attn_mask=mask)  # Attention output\n",
        "        x = self.dropout(self.norm1(attn_output + x))  # Add & Norm 1\n",
        "        forward_output = self.feed_forward(x)  # Feed-forward network\n",
        "        x = self.dropout(self.norm2(forward_output + x))  # Add & Norm 2\n",
        "        return x"
      ],
      "metadata": {
        "id": "NKavnpvoFEXS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model\n"
      ],
      "metadata": {
        "id": "Ecau6AyVFatJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full transformer model with embedding, encoder layers, and final output\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_len):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embed_size)  # Embedding layer\n",
        "        self.position_embedding = PositionalEncoding(embed_size, max_len)  # Positional encoding\n",
        "\n",
        "        # Stack multiple encoder layers\n",
        "        self.layers = nn.ModuleList(\n",
        "            [EncoderLayer(embed_size, heads, forward_expansion, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)  # Final output layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        out = self.dropout(self.word_embedding(x))  # Apply word embedding\n",
        "        out = self.position_embedding(out)  # Add positional encoding\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, mask)  # Pass through each encoder layer\n",
        "        out = self.fc_out(out)  # Generate final output\n",
        "        return out"
      ],
      "metadata": {
        "id": "5v4z_WmZFT6O"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters\n"
      ],
      "metadata": {
        "id": "UPIJB7IrFo8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "VOCAB_SIZE = 50\n",
        "EMBED_SIZE = 512\n",
        "NUM_LAYERS = 3\n",
        "HEADS = 8\n",
        "FORWARD_EXPANSION = 4\n",
        "DROPOUT = 0.1\n",
        "MAX_LEN = 100\n",
        "LR = 0.001"
      ],
      "metadata": {
        "id": "mrqucZ7fFidV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop\n"
      ],
      "metadata": {
        "id": "Lpx9wssLGCC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the transformer model\n",
        "model = Transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embed_size=EMBED_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    heads=HEADS,\n",
        "    forward_expansion=FORWARD_EXPANSION,\n",
        "    dropout=DROPOUT,\n",
        "    max_len=MAX_LEN\n",
        ")\n",
        "\n",
        "# Sample input and target tensors (tokenized sentence)\n",
        "input_tensor = torch.randint(0, VOCAB_SIZE, (1, 5))  # Batch size = 1, seq_len = 5\n",
        "target_tensor = torch.randint(0, VOCAB_SIZE, (1, 5))\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)  # Adam optimizer\n",
        "\n",
        "# Training loop\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(input_tensor, mask=None)  # Forward pass\n",
        "    output = output.view(-1, VOCAB_SIZE)  # Reshape output for loss calculation\n",
        "    target = target_tensor.view(-1)  # Reshape target\n",
        "\n",
        "    loss = criterion(output, target)  # Calculate loss\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')  # Print loss every 100 epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T3Yexk2Fvdk",
        "outputId": "cdda4c29-7248-4bb4-f708-9e8f0fa2dfbf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 4.071639060974121\n",
            "Epoch 100, Loss: 2.9158100005588494e-05\n",
            "Epoch 200, Loss: 2.281638080603443e-05\n",
            "Epoch 300, Loss: 2.0360714188427664e-05\n",
            "Epoch 400, Loss: 2.2196469217306003e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n"
      ],
      "metadata": {
        "id": "0Xw0cwA8GYgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the input\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred = model(input_tensor, mask=None).argmax(dim=2)  # Get predicted tokens\n",
        "    predicted_sentence = \" \".join([str(idx.item()) for idx in pred[0]])  # Convert tokens to sentence\n",
        "    print(f\"Predicted Sentence: {predicted_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9g7CwnxGLcH",
        "outputId": "ae5944c3-197c-4e28-a077-1c0d4841d975"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Sentence: 29 42 7 6 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z7gk0V8wGjhE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}